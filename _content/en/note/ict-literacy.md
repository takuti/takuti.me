---
categories: [Society & Business]
series: [ab]
date: 2026-03-01
lang: en
title: Are You Digitally Prepared?
draft: true
---

Technology is contextual, and hence neither oversimplification nor overcomplication is helpful; it'd be advisable to understand realistic constraints in a very environment and take a tailored approach point by point.

But how can we "see" real challenges and opportunities a person, organization, community, country, or region faces in the information age? Where are *you* currently at, in terms of digital divide?

One way to accomplish this is to travel to a field, spend time with locals, and co-explore problems and solutions. Such fieldwork, however, can be resource-intensive, uncertain, and irrelevant to other contexts. Hence, the work is likely to be unsustainable with minimal scaling-up potential.

That's where assessment frameworks come in.

### Task-based assessment

By applying a standard set of tasks and measurements globally, institutions can gain qualitative and quantitative insights about technology adaptation while eliminating noise. This eases a local reality check and cross-contextual analysis.

For example, [International Computer and Information Literacy Study](https://www.iea.nl/studies/iea/icils/) (ICILS) is a worldwide assessment of ICT literacy in educational settings conducted every five years. They employ their standardized questionnaires and virtual hands-on tasks to measure participants' ability to utilize ICT.

The ICILS reports describe which countries scored higher in information search, use of presentation tools (e.g., PowerPoint), and algorithmic games, as well as how students perceive ICT in their learning journey. It would be a good baseline framework to monitor access and use of digital technologies.

On the other hand, the facilitated approach can decontextualize insights by posing the same problem set for various but not all nations and aggregating the results for the sake of generalization. It won't be fair because each country uses different languages, and students follow different curricula that are likely regulated by the corresponding governments.

By design, assessment in a closed environment introduces many unrealistic factors into the scene, and hence, the results can be skewed toward a narrow definition of literacy.

So, it's overly simplistic to give a designed task and say, "If you can find Text A on a screen, copy and paste it to Box X, and guess what to do next, you are digitally prepared." In fact, it only dictates one's ability to solve specific, well-documented, synthetic tasks.

Since such task-based lab experiments derive a limited understanding of reality, we need [more than tools and HOW-TOs](/note/digital-divide) to bridge the divide fully.

### Situational assessment

Readiness, in reality, is more psychological and situational.

Even if he/she uses a smartphone daily for social media, they won't be prepared to enjoy what ICT offers to the fullest as long as there is psychological friction, such as "Am I doing the right thing?"

One can gain confidence in using technology only when they are in a specific situation, where they can objectively confirm their maturity by seeing others' behaviour.

That's the underlying idea of situational assessment frameworks, like [Technology Readiness Index](https://journals.sagepub.com/doi/10.1177/109467050024001) (TRI) and [TRI 2.0](https://journals.sagepub.com/doi/abs/10.1177/1094670514539730). It focuses on tangible prosperity achieved through the use of technology, not skills.

TRI asks a series of situational questions, which are classified into four categories to measure various contextual factors that promote or limit technology adaptation.

1. **Optimism**: Do you have a positive view of technology?
    - Ex. "Technology gives me more freedom of mobility."
2. **Innovativeness**: Is there a tendency for you to be a leader in tech?
    - Ex. "Other people come to me for advice on new technologies."
3. **Discomfort**: Do you experience the lack of control over tech and feeling uncomfortable?
    - Ex. "In my circle of friends, people are admired more if they own the latest gadgets."
4. **Insecurity**: Do you have skepticism whether tech works appropriately?
    - Ex. "I do not feel confident doing business in an online-only setup."

By asking whether subjects agree or disagree with each statement, we get access to a more nuanced and authentic understanding of the state of individuals' literacy. It has been reported that the framework worked well across diverse contexts.

The invisible factor will differentiate *doing* from *knowing,* which is the key to demonstrate a greater degree of users' agency.

### Toward effective monitoring and evaluation

Notice that both task-based and situation assessments are useful.

Think about driving a car. To obtain a license, you first complete a series of facilitated tasks in a closed setting, such as knowledge acquisition from a textbook, simulation at a driving school, and examination supervised by an authority. All the tasks are essential for you to start driving, yet none of them make you a mature driver.

In order for you to take full advantage of what an automobile offers, you actually need to drive it as frequently and consciously as possible on public roads. From this experiential and situated learning, you develop nuanced and contextualized driving skills, which will tell us whether one is a good driver or not. That's why, even though I have owned my driving license for 14 years without any accident or violation, the lack of "real" driving opportunities in my car-free life under varying conventions between Japan, Canada, and Malawi, put me at the bottom of the pyramid in terms of agency as a driver.

Therefore, while task-based assessment is definitely a good starting point, it would be pointless without situational validation.

Whether it's qualitative or quantitative, and task-based or situational, modern society relies heavily on standardized assessments to visualize reality in an objectively sound way. The measurement tools eventually enable individual agents to monitor the performance of their actions and make informed decisions on what to do next. It also helps stakeholders better control quality and prioritize areas of investment.

In international development, this is often the job of Monitoring & Evaluation (M&E) personnel. However, [my experience in Malawi](/malawi) tells me that assessment methodologies are not structured well, especially in emerging domains like ICT, and tools and languages differ depending on a person in charge; officers are busy with going back and forth between fields and meeting rooms, while they make and distribute questionnaires on an *"as-you-go"* basis with little to no consistency and alignment with larger strategic goals.

In a reciprocal environment like human society, we cannot optimize things we don't measure. And defining the right objective function&mdash;capturing both behavioural and psychological aspects&mdash;is essential for the steady development of any person or organization.
